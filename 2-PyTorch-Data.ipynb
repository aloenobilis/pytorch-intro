{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well be using the MNIST dataset to learn data processing with PyTorch. Torchvision comes prepackaged with datasets. These datasets are pre-pruned for deep learning, their data has been converted to numerical values and there are batches*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With deep learning in general you'll have a training and test dataset, its important to seperate these as early as possible.\n",
    "\n",
    "In order to validate a model we need out of sample test data which is the most realistic test as the model has never seen the data before, as opposed to in sample data that the model was trained on. We can actually test for overfitting by seeing if the model does well on sample data and not well on out of sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MINST dataset, even though it comes from Torchvision is not in Tensor form, transforms allows us to transform the data using built in or custom transform methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True,\n",
    "                  transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = datasets.MNIST(\"\", train=False, download=True,\n",
    "                  transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data in a variable, we can then put it into another object in order to iterate over it.\n",
    "We use the DataLoader to load in data for the model to a variable, specifying batch_size and shuffle.\n",
    "\n",
    "Batch Size is a key topic, a batch size will usually be between 8 and 64 regardless of how big the dataset is. The reasons we use batch sizes is to not only not pass the entire data set through the model at once but also that when we pass our dataset through the model in chunks/batches more generalization occurs and a lot of overfitting dissapears becuase for example when passing lots of mini data sets the model would be better trained. Each datasets batchsize has a sweetspot for a model.\n",
    "\n",
    "Shuffling is extremely important, for example the MNIST digits are in order and the neural network will take shortcuts/tricks to reduce loss (the model has no a priori knowledge of what the lowest loss could be, so as the optimizer is trying to decrease our loss it has no idea of how good of an optimization we could get so it keeps on optimizing by decreasing loss as best/easy as possible), so we shuffle and are always thinking about how to obfuscate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([3, 1, 3, 8, 7, 8, 1, 9, 6, 7])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)             # data is a single batch, 10 examples of hand written digits, with 10 tensors representing the actual output\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python: the temp variable from a for loop can be accessed, here 'data'\n",
    "# data contains a list of images and a list of tensors, x is our image data for corresponsing output tensor y\n",
    "x,y = data[0][0], data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Note:  that pre-loaded datasets are in a strange shape, a normal image would just be 28,28. \n",
    "print(data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data[0][0].view(28,28))  # show image as a 28,28\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure that our dataset is  not overweighted, for example if 60% of MNIST were 3's then the model would always output a 3 and wouldnt know how to predict for other digits. Thats why when creating and exploring a dataset its good to get an idea of how weighted each input is of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1   # at y key, value += 1\n",
    "        total +=1\n",
    "        \n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "for i in counter_dict:\n",
    "    print(f\"{i}: {(counter_dict[i]/total) * 100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

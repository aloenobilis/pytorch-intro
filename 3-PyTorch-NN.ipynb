{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST(\"\", train=True, download=True,\n",
    "                  transform = transforms.Compose([\n",
    "                      transforms.ToTensor()\n",
    "                  ]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True,\n",
    "                  transform = transforms.Compose([\n",
    "                      transforms.ToTensor()\n",
    "                  ]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn provides an OO interface (initialsing) while F is functional (parameters)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# python: Net inherits methods from nn.Module, calling super will call the nn.Module (parent) init function.\n",
    "\n",
    "class Net(nn.Module):\n",
    "    '''\n",
    "    __init__: \n",
    "        - fc1 is fully connected first layer\n",
    "            - the first layer fc1 is input, it outputs to fc2 so we pass the same size from the output of fc1 to the input of fc2\n",
    "            - the output layer fc4 has a size of 10 to represent each of our ten digits\n",
    "        - nn.Linear(input, output), \n",
    "            - input (size of each input sample) are our images, each image is 28 by 28 pixels so a flattend image is 28*28=784\n",
    "            - output (size of each output sample) will have 3 layers of 64 neurons for our hidden layers\n",
    "\n",
    "    forward: \n",
    "        - Most people use the same optimizer function like relu, things change with regards to output, for us the output is multi class and log softmax \n",
    "          works to give a probability distribution. Note that the size of the output is the number if outputs, one for each of our ten digits.\n",
    "        - F.relu() is being run over an entire layers output, relu (rectifide linear) is the activation function or optimizer, it is a sigmoid\n",
    "          for firing or not firing of 'neurons'\n",
    "        - F.log_softmax() takes the output and dimension to output a probability distribution, recall that the goal with output \n",
    "          is to see which neurons fired, so whichever one is closer to 1 then that is the 'most fired' neuron. Because we are dealing with \n",
    "          multiple classes we want a probability distribution on the output.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)   # input to hidden\n",
    "        self.fc2 = nn.Linear(64, 64)      # hidden\n",
    "        self.fc3 = nn.Linear(64, 64)      # hidden\n",
    "        self.fc4 = nn.Linear(64, 10)      # hidden to output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))   # x is the output ( nn.Linear(64, x) ), pass result of fc1 to relu\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)           # for the output we want to output a probability distribution\n",
    "\n",
    "        # probability distribution log_softmax, dimension is 1 as we assume a flat multi dim array tensor for final x (output)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "        \n",
    "net = Net()   \n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1, 28*28)    # -1 tells the reshape/view to expect data at any size Tensor(multi dimen array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1502, -2.3297, -2.3511, -2.3300, -2.3915, -2.2951, -2.1804, -2.2525,\n",
       "         -2.3569, -2.4240]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output # these are the outputs !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.1502, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.max() # max value will be at index, the index is the predicted value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
